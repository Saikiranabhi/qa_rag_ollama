#  RAG Chatbot with FAISS, LangChain, and LLaMA

A Retrieval-Augmented Generation (RAG) chatbot that allows users to upload PDF documents and interactively ask questions about their content. The system leverages state-of-the-art language models, vector search, and modern web technologies to provide accurate, context-aware answers from your documents.

---

## ğŸš€ Features
- **PDF Upload & Parsing:** Upload any PDF and extract its text for analysis.
- **Semantic Search:** Uses FAISS and HuggingFace embeddings for fast, relevant document retrieval.
- **Conversational QA:** Ask questions about your uploaded PDF and get precise answers using LLaMA via Ollama and LangChain.
- **Modern Web UI:** Built with Streamlit for an intuitive, interactive experience.

---

## ğŸ“ Project Structure
```
RAG/
â”œâ”€â”€ app.py                # Main Streamlit application
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ faiss_index/          # Directory for FAISS vector store files
â”‚   â”œâ”€â”€ index.faiss       # FAISS index data
â”‚   â””â”€â”€ index.pkl         # Metadata for the vector store
â”œâ”€â”€ uploaded/             # Uploaded PDF files
â”‚   â”œâ”€â”€ xxx.pdf
â”‚   â””â”€â”€ xxx.pdf
```

---

## ğŸ› ï¸ Technologies Used
- **Python:** Core programming language.
- **Streamlit:** For building the interactive web interface.
- **PyPDF2:** Extracts text from PDF files.
- **LangChain:** Orchestrates the RAG pipeline and integrates LLMs.
- **HuggingFace Transformers:** Provides sentence embeddings for semantic search.
- **FAISS:** Efficient similarity search and vector storage.
- **Ollama:** Runs the LLaMA language model locally for question answering.

### Functionality Overview
- **PDF Parsing:** `PyPDF2` reads and extracts text from uploaded PDFs.
- **Text Chunking:** `LangChain` splits text into manageable chunks for embedding.
- **Embedding & Indexing:** `HuggingFaceEmbeddings` creates vector representations; `FAISS` stores and retrieves them.
- **Retrieval QA:** `LangChain`'s `RetrievalQA` chain fetches relevant chunks and generates answers using LLaMA via `Ollama`.
- **Web App:** `Streamlit` provides file upload, chat interface, and real-time feedback.

---

## âš¡ Getting Started
1. **Clone the repository:**
   git clone https://github.com/yourusername/rag-chatbot.git
   cd rag-chatbot
2. **Install dependencies:**
   pip install -r requirements.txt
3. **Run the app:**
   streamlit run app.py
4. **Open in browser:**
   Visit [http://localhost:8501](http://localhost:8501) to use the chatbot.

---

## ğŸ“ Example Usage
1. Upload a PDF file using the sidebar.
2. Wait for the system to process and index the document.
3. Type your question in the chat box (e.g., "What is the main topic of this document?").
4. Receive an answer generated from the content of your PDF.

---

## ğŸ”— Links
- **GitHub Repository:** [https://github.com/yourusername/rag-chatbot](https://github.com/yourusername/rag-chatbot)
- **Live Demo:** [https://your-demo-link.com](https://your-demo-link.com)

---

## ğŸ“š More Information
- **Extensible:** Easily swap out the LLM, embeddings, or vector store for your needs.
- **Local & Private:** All processing is done locally; your documents never leave your machine.
- **Customizable:** Adapt the chunk size, overlap, or model for different document types.

---

## ğŸ“„ License
This project is licensed under the MIT License. 
